import requests
from bs4 import BeautifulSoup
import os
import re
import json

def mkdir(path):
  '''
  åˆ›å»ºæ–‡ä»¶å¤¹
  '''
  folder = os.path.exists(path)
  if not folder:  # åˆ¤æ–­æ˜¯å¦å­˜åœ¨æ–‡ä»¶å¤¹å¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºä¸ºæ–‡ä»¶å¤¹
      print("---  åˆ›å»ºæ–°çš„æ–‡ä»¶å¤¹ğŸ˜€  ---")
      os.makedirs(path)  # makedirs åˆ›å»ºæ–‡ä»¶æ—¶å¦‚æœè·¯å¾„ä¸å­˜åœ¨ä¼šåˆ›å»ºè¿™ä¸ªè·¯å¾„
      print("---  OK ğŸš© ---")
  else:
      print("--- âš ï¸ æ–‡ä»¶å¤¹å·²å­˜åœ¨!  ---")

def fetchUrl(url):
    '''
    å‘èµ·ç½‘ç»œè¯·æ±‚ï¼Œè·å–ç½‘é¡µæºç 
    '''
    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9 ',
        'cookie':'', # æ¢æˆè‡ªå·±çš„cookieå“¦~
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 UBrowser/6.2.4098.3 Safari/537.36',
    }
    
    r = requests.get(url, headers = headers)
    return r.text

def parsing_link(html):
    '''
    è§£æhtmlæ–‡æœ¬ï¼Œæå–æ— æ°´å°å›¾ç‰‡çš„ url
    '''
    soup = BeautifulSoup(html, 'html.parser')
    script = soup.find('script', string=re.compile('window\.__INITIAL_STATE__'))

    test = re.split(r'=', script.string)
    # å¤„ç†å­—ç¬¦ä¸²jsonæ•°æ®ä¸åˆç†çš„åœ°æ–¹
    string = test[1].replace('undefined', 'null') 
    # è½¬æ¢æˆjsonæ•°æ®
    result = json.loads(string, strict=False)
    # è·å–å¯¹åº”å­—æ®µ
    imageList = result['note']['note']['imageList']
    title = result['note']['note']['title']
    print('æ ‡é¢˜ï¼š', title)
    print('å¼€å§‹ä¸‹è½½å•¦ï¼ğŸš€')
    
    # è°ƒç”¨ç”Ÿæˆä»¥titleä¸ºåçš„æ–‡ä»¶å¤¹, å¯è‡ªå®šä¹‰è¦ä¿å­˜çš„è·¯å¾„
    file = os.path.dirname(__file__) + '/image/' + title
    mkdir(file) 
    
    # æå–å›¾ç‰‡
    for i in imageList:
        picUrl = f"https://sns-img-qc.xhscdn.com/{i['traceId']}"
        yield picUrl, i['traceId'], title

def download(url, filename, folder):
    '''
    ä¸‹è½½å›¾ç‰‡
    '''
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 UBrowser/6.2.4098.3 Safari/537.36',
    }

    with open(f'image/{folder}/{filename}.jpg', 'wb') as v:
        try:
            r = requests.get(url, headers=headers)
            v.write(r.content)
        except Exception as e:
            print('å›¾ç‰‡ä¸‹è½½é”™è¯¯ï¼')

def roopLink(urls):
  '''
  éå†urls,æ‰¹é‡ä¸‹è½½å»æ°´å°å›¾ç‰‡
  '''
  for item in urls:
    html = fetchUrl(item)
    for url, traceId, title in parsing_link(html):
        print(f"download image {url}")
       
        download(url, traceId, title)

if __name__ == '__main__':
    # è¾“å…¥å°çº¢ä¹¦çš„é“¾æ¥
    links = ['https://www.xiaohongshu.com/explore/63f07247000000001300d67b','https://www.xiaohongshu.com/explore/60a5f16f0000000021034cb4']
    roopLink(links)
    print("Finished!ğŸ‰")
