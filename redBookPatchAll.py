import requests
from bs4 import BeautifulSoup
import os
import re
import json
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver import ChromeOptions
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

def mkdir(path):
  '''
  åˆ›å»ºæ–‡ä»¶å¤¹
  '''
  folder = os.path.exists(path)
  if not folder:  # åˆ¤æ–­æ˜¯å¦å­˜åœ¨æ–‡ä»¶å¤¹å¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºä¸ºæ–‡ä»¶å¤¹
      print("---  åˆ›å»ºæ–°çš„æ–‡ä»¶å¤¹ğŸ˜€  ---")
      os.makedirs(path)  # makedirs åˆ›å»ºæ–‡ä»¶æ—¶å¦‚æœè·¯å¾„ä¸å­˜åœ¨ä¼šåˆ›å»ºè¿™ä¸ªè·¯å¾„
      print("---  OK ğŸš© ---")
  else:
      print("--- âš ï¸ æ–‡ä»¶å¤¹å·²å­˜åœ¨!  ---")

def fetchUrl(url):
    '''
    å‘èµ·ç½‘ç»œè¯·æ±‚ï¼Œè·å–ç½‘é¡µæºç 
    '''
    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9 ',
        'cookie':'xhsTrackerId=7ff0cee2-d318-4845-a5f1-7f4fb9acb9c3; xhsTrackerId.sig=hrWK6Hw0SUZ8mRCiVj0KO1K4nyb5Rbr3cxcAneVDyJY; a1=186823c9460khhoyfzypkmiw8cprofje475xhyso150000115335; webId=bbb653427b6f8ad0d223ff07d448275f; gid=yYKYJqSjjJ4dyYKYJqSj4WvMK8TxxlviuvUT2ly63EYSUC28FliVdx888yy2qq28j0DDy2dS; gid.sign=SVNqT90rFZJP8Hpu1fDr6lxAv7Q=; web_session=030037a4cb351b0516ff232780244a0727192f; customerClientId=918124472893177; x-user-id-ark.xiaohongshu.com=62b982519a415e00014f6c2f; timestamp2=1677477939195dfe19d311adbcb4966d1bc2c3da33a8ec424ce52a391b32474; timestamp2.sig=RuqNBEIFoHscqX8BtjGLcla6Yn5Z36oIRc4hMvXN1iI; gr_user_id=711f3d34-fd16-4ba2-bc61-f09ca8023256; x-user-id-eva.xiaohongshu.com=62b982519a415e00014f6c2f; xhsTracker=url=user-profile&xhsshare=CopyLink; xhsTracker.sig=WS8d3HYlzoIfhHjyJtY_Y1QP5iYacJ96TpUFr1hgfm4; extra_exp_ids=yamcha_0327_exp,h5_1208_exp3; extra_exp_ids.sig=ANlofVKSDcIxHrXW_rvDettMT1wABiN2baUCClhZnYI; webBuild=2.0.3; websectiga=82e85efc5500b609ac1166aaf086ff8aa4261153a448ef0be5b17417e4512f28; sec_poison_id=18a62e3c-9284-4e5d-a196-c777ed2a4c6a; xsecappid=yamcha',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',
    }
    
    r = requests.get(url, headers = headers)
    return r.text

def parsing_link(html):
    '''
    è§£æhtmlæ–‡æœ¬ï¼Œæå–æ— æ°´å°å›¾ç‰‡çš„ url
    '''
    soup = BeautifulSoup(html, 'html.parser')
    script = soup.find('script', string=re.compile('window\.__INITIAL_STATE__'))

    # print(script.string, 'ğŸš€ğŸš€ğŸš€ğŸš€')
    test = re.split(r'_=', script.string)
    # å¤„ç†å­—ç¬¦ä¸²jsonæ•°æ®ä¸åˆç†çš„åœ°æ–¹
    string = test[1].replace('undefined', 'null') 
    # è½¬æ¢æˆjsonæ•°æ®
    result = json.loads(string, strict=False)
    # # è·å–å¯¹åº”å­—æ®µ
    video = ''
    videoId = ''
    imageList = []

    noteDetailMap = result['note']['noteDetailMap']
    # è·å–ç¬¬ä¸€ä¸ªé”®
    first_key = next(iter(noteDetailMap))
    first_value = noteDetailMap[first_key]
    if 'video' in first_value['note'] :
        video = first_value['note']['video']['media']['stream']['h264'][0]['masterUrl']
        videoId = first_value['note']['video']['media']['videoId']
    else:
        imageList = first_value['note']['imageList']
    
    title = first_value['note']['title']
    print('æ ‡é¢˜ï¼š', title)
    print('å¼€å§‹ä¸‹è½½å•¦ï¼ğŸš€')

    # # è°ƒç”¨ç”Ÿæˆä»¥titleä¸ºåçš„æ–‡ä»¶å¤¹, å¯è‡ªå®šä¹‰è¦ä¿å­˜çš„è·¯å¾„
    file = os.path.dirname(__file__) + '/image/' + title
    mkdir(file) 

    print(video, 'ğŸš€ğŸš€ğŸš€ğŸš€')
    if video:
        downloadVideo(video, videoId, title)
    # æå–å›¾ç‰‡
    for index, i in enumerate(imageList):
        picUrl = i['urlDefault']
        yield picUrl, index, title

def download(url, filename, folder):
    '''
    ä¸‹è½½å›¾ç‰‡
    '''
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 UBrowser/6.2.4098.3 Safari/537.36',
    }

    with open(f'image/{folder}/{filename}.jpg', 'wb') as v:
        try:
            r = requests.get(url, headers=headers)
            v.write(r.content)
        except Exception as e:
            print('å›¾ç‰‡ä¸‹è½½é”™è¯¯ï¼')

def downloadVideo(url, filename, folder):
    '''
    ä¸‹è½½è§†é¢‘
    '''
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 UBrowser/6.2.4098.3 Safari/537.36',
    }

    with open(f'image/{folder}/{filename}.mp4', 'wb') as v:
        try:
            r = requests.get(url, headers=headers)
            v.write(r.content)
        except Exception as e:
            print('è§†é¢‘ä¸‹è½½é”™è¯¯ï¼')

def roopLink(urls):
  '''
  éå†urls,æ‰¹é‡ä¸‹è½½å»æ°´å°å›¾ç‰‡
  '''
  for item in urls:
    html = fetchUrl(item)
    parsing_link(html)
    for url, traceId, title in parsing_link(html):
        print(f"download image {url}")
       
        download(url, traceId, title)

if __name__ == '__main__':
  

    option = ChromeOptions()
    option.add_experimental_option('excludeSwitches', ['enable-automation','enable-logging'])
    option.add_argument("--disable-blink-features")
    option.add_argument("--disable-blink-features=AutomationControlled")
    option.add_experimental_option("detach", True)

    desired_capabilities = DesiredCapabilities.CHROME
    desired_capabilities["pageLoadStrategy"] = "none"

    # æŒ‡å®š chromedriver çš„è·¯å¾„
    chromedriver_path = '/Users/hongdazhu/chromedriver/120/chromedriver'

    # åˆ›å»ºChromeæµè§ˆå™¨å¯¹è±¡
    browser = webdriver.Chrome(executable_path=chromedriver_path,options = option)
    # version = browser.capabilities['browserVersion']
    # print(version, 'versionversion')

    # å°çº¢ä¹¦ä¸»é¡µçš„åœ°å€
    urls = [
        'https://www.xiaohongshu.com/user/profile/5f2e6cb800000000010063e2?m_source=pinpai',
        # å¯ä»¥æ·»åŠ æ›´å¤šçš„é“¾æ¥
        # 'https://www.xiaohongshu.com/user/profile/å¦ä¸€ä¸ªç”¨æˆ·çš„ID',
    ]

    for url in urls:
        browser.get(url)

        # è®¾ç½®éšå¼ç­‰å¾…æ—¶é—´ä¸º10ç§’
        time.sleep(3)
        browser.refresh()
        time.sleep(5)
    
        pages = browser.page_source
        soup = BeautifulSoup(pages, 'html.parser')

        postId = []
        hrefArr = []

        for span in soup.find_all('a', class_='cover ld mask'):
            # titles.append(span.find('h2').text)
            postId.append('https://www.xiaohongshu.com/explore/'+span.get('href').split('/')[4])
        
        # print(soup, 'noteItem')
        print(postId)

        roopLink(postId)
    print("Finished!ğŸ‰")
